{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0644406",
      "metadata": {
        "id": "f0644406"
      },
      "source": [
        "# Настраевае среду colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725bba04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "725bba04",
        "outputId": "68aeb125-85df-42f2-f33e-98e68a4b7b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'counterfeit-ozon-ml'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 62 (delta 23), reused 48 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (62/62), 38.89 KiB | 7.78 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/meineac/counterfeit-ozon-ml.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd counterfeit-ozon-ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ujZrsHSxpH5",
        "outputId": "e279346a-6c97-4d3c-9b95-6b91ebe66b87"
      },
      "id": "6ujZrsHSxpH5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/counterfeit-ozon-ml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6UTiwGLxu8x",
        "outputId": "c03b6eaf-bd4e-46da-f771-5748b96c28d7"
      },
      "id": "N6UTiwGLxu8x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03733eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c03733eb",
        "outputId": "aad5a4b0-ae3f-4598-f502-0c3adb42ce9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.6.0)\n",
            "Collecting rapidfuzz (from -r requirements.txt (line 8))\n",
            "  Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting datetime (from -r requirements.txt (line 9))\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting optuna (from -r requirements.txt (line 10))\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Collecting zope.interface (from datetime->-r requirements.txt (line 9))\n",
            "  Downloading zope.interface-7.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna->-r requirements.txt (line 10))\n",
            "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna->-r requirements.txt (line 10))\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->-r requirements.txt (line 10)) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna->-r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna->-r requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna->-r requirements.txt (line 10)) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->-r requirements.txt (line 10)) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->-r requirements.txt (line 10)) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from zope.interface->datetime->-r requirements.txt (line 9)) (75.2.0)\n",
            "Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading zope.interface-7.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, rapidfuzz, colorlog, datetime, alembic, optuna\n",
            "Successfully installed alembic-1.16.5 colorlog-6.9.0 datetime-5.5 optuna-4.5.0 rapidfuzz-3.14.0 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4DFXDjl0lX0",
        "outputId": "55b9db88-6282-4481-80ac-9f19be4b3b1d"
      },
      "id": "a4DFXDjl0lX0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/raw\n",
        "!cp \"/content/drive/MyDrive/OZON_ML/ml_ozon_сounterfeit_data/ml_ozon_сounterfeit_test_new.csv\" data/raw/ml_ozon_counterfeit_test_new.csv\n",
        "!cp \"/content/drive/MyDrive/OZON_ML/ml_ozon_сounterfeit_data/ml_ozon_сounterfeit_train.csv\" data/raw/ml_ozon_counterfeit_train.csv\n"
      ],
      "metadata": {
        "id": "-csMBQYc0jkt"
      },
      "id": "-csMBQYc0jkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/OZON_ML/ml_ozon_сounterfeit_data/ml_ozon_сounterfeit_test.csv\" data/raw/ml_ozon_counterfeit_test.csv"
      ],
      "metadata": {
        "id": "xjFDtrZV0eII"
      },
      "id": "xjFDtrZV0eII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7e1f3bed",
      "metadata": {
        "id": "7e1f3bed"
      },
      "source": [
        "# Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5badf6c9",
      "metadata": {
        "id": "5badf6c9"
      },
      "source": [
        "## Загрузка данных train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7fb084",
      "metadata": {
        "id": "3c7fb084",
        "outputId": "cb65876e-0b12-4ce3-a941-181945d90cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Тренировочный файл успешно загружен.\n",
            "\n",
            "--- 1. Размерность данных ---\n",
            "Форма датасета: (197198, 45)\n",
            "\n",
            "--- 2. Общая информация и типы данных ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 197198 entries, 0 to 197197\n",
            "Data columns (total 45 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   id                            197198 non-null  int64  \n",
            " 1   resolution                    197198 non-null  int64  \n",
            " 2   brand_name                    116667 non-null  object \n",
            " 3   description                   171138 non-null  object \n",
            " 4   name_rus                      197198 non-null  object \n",
            " 5   CommercialTypeName4           197198 non-null  object \n",
            " 6   rating_1_count                47193 non-null   float64\n",
            " 7   rating_2_count                47193 non-null   float64\n",
            " 8   rating_3_count                47193 non-null   float64\n",
            " 9   rating_4_count                47193 non-null   float64\n",
            " 10  rating_5_count                47193 non-null   float64\n",
            " 11  comments_published_count      47193 non-null   float64\n",
            " 12  photos_published_count        47193 non-null   float64\n",
            " 13  videos_published_count        47193 non-null   float64\n",
            " 14  PriceDiscounted               197198 non-null  float64\n",
            " 15  item_time_alive               197198 non-null  int64  \n",
            " 16  item_count_fake_returns7      197198 non-null  int64  \n",
            " 17  item_count_fake_returns30     197198 non-null  int64  \n",
            " 18  item_count_fake_returns90     197198 non-null  int64  \n",
            " 19  item_count_sales7             197198 non-null  int64  \n",
            " 20  item_count_sales30            197198 non-null  int64  \n",
            " 21  item_count_sales90            197198 non-null  int64  \n",
            " 22  item_count_returns7           197198 non-null  int64  \n",
            " 23  item_count_returns30          197198 non-null  int64  \n",
            " 24  item_count_returns90          197198 non-null  int64  \n",
            " 25  GmvTotal7                     187007 non-null  float64\n",
            " 26  GmvTotal30                    189268 non-null  float64\n",
            " 27  GmvTotal90                    189791 non-null  float64\n",
            " 28  ExemplarAcceptedCountTotal7   187007 non-null  float64\n",
            " 29  ExemplarAcceptedCountTotal30  189268 non-null  float64\n",
            " 30  ExemplarAcceptedCountTotal90  189791 non-null  float64\n",
            " 31  OrderAcceptedCountTotal7      186797 non-null  float64\n",
            " 32  OrderAcceptedCountTotal30     189038 non-null  float64\n",
            " 33  OrderAcceptedCountTotal90     189681 non-null  float64\n",
            " 34  ExemplarReturnedCountTotal7   187007 non-null  float64\n",
            " 35  ExemplarReturnedCountTotal30  189268 non-null  float64\n",
            " 36  ExemplarReturnedCountTotal90  189791 non-null  float64\n",
            " 37  ExemplarReturnedValueTotal7   187007 non-null  float64\n",
            " 38  ExemplarReturnedValueTotal30  189268 non-null  float64\n",
            " 39  ExemplarReturnedValueTotal90  189791 non-null  float64\n",
            " 40  ItemVarietyCount              196201 non-null  float64\n",
            " 41  ItemAvailableCount            196201 non-null  float64\n",
            " 42  seller_time_alive             197198 non-null  float64\n",
            " 43  ItemID                        197198 non-null  int64  \n",
            " 44  SellerID                      197198 non-null  int64  \n",
            "dtypes: float64(27), int64(14), object(4)\n",
            "memory usage: 67.7+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_folder = 'data/raw/'\n",
        "file_name = 'ml_ozon_counterfeit_train.csv'\n",
        "file_path = file_folder + file_name\n",
        "\n",
        "try:\n",
        "    df_train = pd.read_csv(file_path)\n",
        "    print(\"✅ Тренировочный файл успешно загружен.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Ошибка: Файл не найден по пути '{file_path}'. Проверьте путь.\")\n",
        "\n",
        "if 'df_train' in locals():\n",
        "    print(\"\\n--- 1. Размерность данных ---\")\n",
        "    print(f\"Форма датасета: {df_train.shape}\")\n",
        "\n",
        "    print(\"\\n--- 2. Общая информация и типы данных ---\")\n",
        "    # .info() показывает типы данных и количество НЕпустых значений\n",
        "    df_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dfa956d",
      "metadata": {
        "id": "3dfa956d"
      },
      "source": [
        "## Предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2005d7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2005d7e",
        "outputId": "2d1c880a-d118-432d-a3d0-519119e21996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Начинаю Шаг 3: Предобработка и инжиниринг признаков ---\n",
            "1. Заполняю пропуски...\n",
            "   ...пропуски заполнены.\n",
            "2. Очищаю текстовые поля от HTML-тегов...\n",
            "   ...текст очищен.\n",
            "3. Создаю новые признаки...\n",
            "   ...новые признаки созданы.\n",
            "\n",
            "--- Проверка результата ---\n",
            "Оставшиеся пропуски:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "✅ Шаг 3 успешно завершен!\n"
          ]
        }
      ],
      "source": [
        "import re # Импортируем модуль для работы с регулярными выражениями\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    print(\"--- Начинаю Шаг 3: Предобработка и инжиниринг признаков ---\")\n",
        "\n",
        "    # Создаем копию, чтобы не изменять оригинальный датафрейм\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # --- 1. Обработка пропущенных значений ---\n",
        "    print(\"1. Заполняю пропуски...\")\n",
        "\n",
        "    # Блок с рейтингами и отзывами (NaN -> 0)\n",
        "    rating_cols = [col for col in df_processed.columns if 'rating' in col or 'count' in col]\n",
        "    # Уточняем список, чтобы не затронуть лишние столбцы\n",
        "    cols_to_fill_zero = [\n",
        "        'rating_1_count', 'rating_2_count', 'rating_3_count', 'rating_4_count', 'rating_5_count',\n",
        "        'comments_published_count', 'photos_published_count', 'videos_published_count',\n",
        "        'GmvTotal7', 'GmvTotal30', 'GmvTotal90', 'ExemplarAcceptedCountTotal7',\n",
        "        'ExemplarAcceptedCountTotal30', 'ExemplarAcceptedCountTotal90', 'OrderAcceptedCountTotal7',\n",
        "        'OrderAcceptedCountTotal30', 'OrderAcceptedCountTotal90', 'ExemplarReturnedCountTotal7',\n",
        "        'ExemplarReturnedCountTotal30', 'ExemplarReturnedCountTotal90', 'ExemplarReturnedValueTotal7',\n",
        "        'ExemplarReturnedValueTotal30', 'ExemplarReturnedValueTotal90', 'ItemVarietyCount', 'ItemAvailableCount'\n",
        "    ]\n",
        "    for col in cols_to_fill_zero:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = df_processed[col].fillna(0)\n",
        "\n",
        "    # Категориальные признаки\n",
        "    df_processed['brand_name'] = df_processed['brand_name'].fillna('_UNKNOWN_')\n",
        "    df_processed['description'] = df_processed['description'].fillna('no_description')\n",
        "\n",
        "    print(\"   ...пропуски заполнены.\")\n",
        "\n",
        "    # --- 2. Очистка текста ---\n",
        "    print(\"2. Очищаю текстовые поля от HTML-тегов...\")\n",
        "    def clean_html(text):\n",
        "        if isinstance(text, str):\n",
        "            # Удаляем HTML-теги\n",
        "            clean_text = re.sub(r'<.*?>', ' ', text)\n",
        "            # Удаляем переносы строк и лишние пробелы\n",
        "            clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "            return clean_text\n",
        "        return text\n",
        "\n",
        "    df_processed['description_cleaned'] = df_processed['description'].apply(clean_html)\n",
        "    df_processed['name_rus_cleaned'] = df_processed['name_rus'].apply(clean_html)\n",
        "    print(\"   ...текст очищен.\")\n",
        "\n",
        "    # --- 3. Feature Engineering ---\n",
        "    print(\"3. Создаю новые признаки...\")\n",
        "\n",
        "    # Признаки на основе длины текста\n",
        "    df_processed['description_len'] = df_processed['description_cleaned'].str.len()\n",
        "    df_processed['name_rus_len'] = df_processed['name_rus_cleaned'].str.len()\n",
        "\n",
        "    # Признаки-отношения (добавляем epsilon для избежания деления на ноль)\n",
        "    epsilon = 1e-6\n",
        "    df_processed['return_to_sales_ratio_90'] = df_processed['item_count_returns90'] / (df_processed['item_count_sales90'] + epsilon)\n",
        "    df_processed['fake_return_ratio_90'] = df_processed['item_count_fake_returns90'] / (df_processed['item_count_returns90'] + epsilon)\n",
        "\n",
        "    # Признак \"есть ли у товара отзывы\"\n",
        "    df_processed['has_reviews'] = (df_processed['rating_1_count'] + df_processed['rating_5_count'] > 0).astype(int)\n",
        "    print(\"   ...новые признаки созданы.\")\n",
        "\n",
        "    print(\"\\n--- Проверка результата ---\")\n",
        "    # Проверяем, что пропусков не осталось (кроме тех, где изначально не было)\n",
        "    remaining_na = df_processed.isnull().sum()\n",
        "    print(\"Оставшиеся пропуски:\")\n",
        "    print(remaining_na[remaining_na > 0])\n",
        "    print(\"\\n✅ Шаг 3 успешно завершен!\")\n",
        "    return df_processed\n",
        "\n",
        "if 'df_train' in locals():\n",
        "    df_processed = preprocess_data(df_train)\n",
        "else:\n",
        "    print(\"❌ Переменная 'df_train' не найдена.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e90309",
      "metadata": {
        "id": "e4e90309"
      },
      "source": [
        "### Сохраняем обработанные данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7deee34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7deee34",
        "outputId": "9409d049-4f86-4daa-c74e-5be5fea97ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Обработанные данные успешно сохранены в файл:\n",
            "   -> /content/counterfeit-ozon-ml/data/processed/df_processed.feather\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if 'df_processed' in locals():\n",
        "    # --- ШАГ 1: Определяем путь для сохранения ---\n",
        "    # Создаем папку 'data/processed/', если она еще не существует\n",
        "    processed_data_path = '/content/counterfeit-ozon-ml/data/processed/'\n",
        "    os.makedirs(processed_data_path, exist_ok=True)\n",
        "\n",
        "    # Имя файла\n",
        "    output_file_path = os.path.join(processed_data_path, 'df_processed.feather')\n",
        "\n",
        "    # --- ШАГ 2: Сохраняем DataFrame ---\n",
        "    try:\n",
        "        # Feather требует сбросить индекс, если он не является стандартным RangeIndex\n",
        "        df_processed.reset_index(drop=True).to_feather(output_file_path)\n",
        "        print(f\"✅ Обработанные данные успешно сохранены в файл:\")\n",
        "        print(f\"   -> {output_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Произошла ошибка при сохранении файла: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Переменная 'df_processed' не найдена. Пожалуйста, выполните предыдущий шаг.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c102d3f",
      "metadata": {
        "id": "9c102d3f"
      },
      "source": [
        "## Расшириные признаки"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0b390f",
      "metadata": {
        "id": "2e0b390f"
      },
      "source": [
        "## Обучение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129576f7",
      "metadata": {
        "id": "129576f7"
      },
      "source": [
        "### # --- 1. Подготовка данных ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f16946a",
      "metadata": {
        "id": "5f16946a",
        "outputId": "b9346a9d-ea3c-4757-c47b-8c897bbdd126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ...создаю продвинутые признаки...\n",
            "   ...продвинутые признаки созданы.\n",
            "\n",
            "--- Перезапускаю обучение ансамбля с новыми признаками ---\n"
          ]
        }
      ],
      "source": [
        "# --- Применяем новую функцию к нашим данным ---\n",
        "if 'df_processed' in locals():\n",
        "    df_processed_advanced = create_advanced_features(df_processed)\n",
        "\n",
        "    print(\"\\n--- Перезапускаю обучение ансамбля с новыми признаками ---\")\n",
        "\n",
        "    df_processed_advanced['text_features'] = df_processed_advanced['name_rus_cleaned'] + ' ' + df_processed_advanced['description_cleaned']\n",
        "    y = df_processed_advanced['resolution']\n",
        "\n",
        "    features_to_drop = ['resolution', 'id', 'description', 'name_rus', 'description_cleaned', 'name_rus_cleaned', 'text_features']\n",
        "    X_tab = df_processed_advanced.drop(columns=features_to_drop)\n",
        "    categorical_features = ['brand_name', 'CommercialTypeName4']\n",
        "    for col in categorical_features:\n",
        "        X_tab[col] = X_tab[col].astype('category')\n",
        "\n",
        "    X_text = df_processed_advanced['text_features']\n",
        "else:\n",
        "    print(\"❌ Переменная 'df_processed' не найдена.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "345c75da",
      "metadata": {
        "id": "345c75da"
      },
      "source": [
        "### --- 2. Кросс-валидация ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e12196c",
      "metadata": {
        "id": "6e12196c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "68e9e2d4-d1d8-42fa-e2a1-530bca2b04dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Получаю OOF-предсказания на 5 фолдах (LGBM будет использовать новые фичи)...\n",
            "--- Фолд 1/5 ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1894094568.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mneg_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlgbm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpos_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_leaves\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlgbm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moof_lgbm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgbm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1561\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4153\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m             _safe_call(\n\u001b[0;32m-> 4155\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "required_vars = ['y', 'X_tab', 'X_text', 'categorical_features']\n",
        "error = [var for var in required_vars if var not in locals()]\n",
        "\n",
        "if not error:\n",
        "    N_SPLITS = 5\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "    oof_lgbm = np.zeros(len(X_tab))\n",
        "    oof_text = np.zeros(len(X_tab))\n",
        "\n",
        "    print(f\"\\nПолучаю OOF-предсказания на {N_SPLITS} фолдах (LGBM будет использовать новые фичи)...\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_tab, y)):\n",
        "        print(f\"--- Фолд {fold+1}/{N_SPLITS} ---\")\n",
        "\n",
        "        # Обучение LightGBM\n",
        "        X_train_tab, y_train = X_tab.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_val_tab, y_val = X_tab.iloc[val_idx], y.iloc[val_idx]\n",
        "        neg_count, pos_count = y_train.value_counts()\n",
        "        lgbm_model = LGBMClassifier(random_state=42, scale_pos_weight=neg_count / pos_count, n_estimators=1000, learning_rate=0.05, num_leaves=31)\n",
        "        lgbm_model.fit(X_train_tab, y_train, eval_set=[(X_val_tab, y_val)], eval_metric='f1', callbacks=[], categorical_feature=categorical_features)\n",
        "        oof_lgbm[val_idx] = lgbm_model.predict_proba(X_val_tab)[:, 1]\n",
        "\n",
        "        # Обучение Text Pipeline\n",
        "        X_train_text, X_val_text = X_text.iloc[train_idx], X_text.iloc[val_idx]\n",
        "        text_pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=3, max_df=0.9)),\n",
        "            ('logreg', LogisticRegression(C=5, class_weight='balanced', random_state=42, solver='liblinear'))\n",
        "        ])\n",
        "        text_pipeline.fit(X_train_text, y_train)\n",
        "        oof_text[val_idx] = text_pipeline.predict_proba(X_val_text)[:, 1]\n",
        "else:\n",
        "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a397e8e6",
      "metadata": {
        "id": "a397e8e6"
      },
      "source": [
        "### --- 3. Поиск лучшего веса и порога ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e1e0e6",
      "metadata": {
        "id": "41e1e0e6",
        "outputId": "231df930-4798-40be-f4e3-2a228322c549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Подбираю лучший вес и порог для НОВОГО ансамбля...\n",
            "\n",
            "==================================================\n",
            "Подбор для УЛУЧШЕННОЙ модели завершен!\n",
            "F1-score УЛУЧШЕННОЙ табличной модели (LGBM): 0.8007\n",
            "F1-score текстовой модели (LogReg): 0.6260\n",
            "--------------------\n",
            "Лучший F1-score НОВОГО ансамбля: 0.8328\n",
            "Лучший вес для LGBM: 0.70\n",
            "Лучший порог: 0.70\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "required_vars = ['oof_lgbm', 'oof_text']\n",
        "error = [var for var in required_vars if var not in locals()]\n",
        "\n",
        "if not error:\n",
        "    print(\"\\nПодбираю лучший вес и порог для НОВОГО ансамбля...\")\n",
        "    best_f1, best_weight, best_threshold = 0, 0, 0\n",
        "    for weight in np.arange(0.6, 1.01, 0.05):\n",
        "        blended_oof = weight * oof_lgbm + (1 - weight) * oof_text\n",
        "        for threshold in np.arange(0.1, 0.91, 0.05):\n",
        "            preds = (blended_oof > threshold).astype(int)\n",
        "            f1 = f1_score(y, preds)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_weight, best_threshold = f1, weight, threshold\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Подбор для УЛУЧШЕННОЙ модели завершен!\")\n",
        "    print(f\"F1-score УЛУЧШЕННОЙ табличной модели (LGBM): {f1_score(y, (oof_lgbm > 0.5).astype(int)):.4f}\")\n",
        "    print(f\"F1-score текстовой модели (LogReg): {f1_score(y, (oof_text > 0.5).astype(int)):.4f}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Лучший F1-score НОВОГО ансамбля: {best_f1:.4f}\")\n",
        "    print(f\"Лучший вес для LGBM: {best_weight:.2f}\")\n",
        "    print(f\"Лучший порог: {best_threshold:.2f}\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тонкая настройка гиперпараметров с помощью Optuna"
      ],
      "metadata": {
        "id": "QAlmy1pZ7aiH"
      },
      "id": "QAlmy1pZ7aiH"
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from lightgbm import LGBMClassifier\n",
        "import numpy as np\n",
        "import joblib  # Для сохранения/загрузки study\n",
        "import json    # Для сохранения параметров\n",
        "import os\n",
        "\n",
        "if 'df_processed_advanced' in locals() and 'X_tab' in locals() and 'y' in locals():\n",
        "    print(\"--- Начинаю Этап 1: Поиск лучших гиперпараметров для LightGBM с Optuna ---\")\n",
        "\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'objective': 'binary', 'metric': 'f1', 'verbosity': -1,\n",
        "            'boosting_type': 'gbdt', 'random_state': 42, 'n_estimators': 1000,\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        }\n",
        "        neg_count, pos_count = y.value_counts()\n",
        "        params['scale_pos_weight'] = neg_count / pos_count\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        f1_scores = []\n",
        "        for train_idx, val_idx in skf.split(X_tab, y):\n",
        "            X_train, y_train = X_tab.iloc[train_idx], y.iloc[train_idx]\n",
        "            X_val, y_val = X_tab.iloc[val_idx], y.iloc[val_idx]\n",
        "            model = LGBMClassifier(**params)\n",
        "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='f1', callbacks=[])\n",
        "            preds = model.predict(X_val)\n",
        "            f1_scores.append(f1_score(y_val, preds))\n",
        "        return np.mean(f1_scores)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Поиск завершен!\")\n",
        "    print(f\"Лучший F1-score: {study.best_value:.4f}\")\n",
        "    print(\"Лучшие гиперпараметры:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # --- 2. Сохранение результатов ---\n",
        "    print(\"\\n--- Сохраняю результаты исследования ---\")\n",
        "\n",
        "    # Путь для сохранения на Google Drive\n",
        "    save_path = '/content/drive/MyDrive/Colab Notebooks/OzonContest'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Сохраняем весь объект study\n",
        "    study_file = os.path.join(save_path, 'optuna_study.pkl')\n",
        "    joblib.dump(study, study_file)\n",
        "    print(f\"✅ Полное исследование сохранено в: {study_file}\")\n",
        "\n",
        "    # Сохраняем лучшие параметры в json\n",
        "    params_file = os.path.join(save_path, 'best_lgbm_params.json')\n",
        "    with open(params_file, 'w') as f:\n",
        "        json.dump(study.best_params, f, indent=4)\n",
        "    print(f\"✅ Лучшие параметры сохранены в: {params_file}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Необходимые данные (df_processed_advanced, X_tab, y) не найдены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "OTu6FXO47XLw",
        "outputId": "76e2a28c-4883-4ed9-c86a-218dbed101e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'optuna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1960469449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "id": "OTu6FXO47XLw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Применение оптимизированных параметров и оценка"
      ],
      "metadata": {
        "id": "Ut_VPhawJWtq"
      },
      "id": "Ut_VPhawJWtq"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Загрузка лучших параметров ---\n",
        "print(\"--- Загружаю лучшие параметры, найденные Optuna ---\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    params_file = '/content/drive/MyDrive/Colab Notebooks/OzonContest/best_lgbm_params.json'\n",
        "    with open(params_file, 'r') as f:\n",
        "        best_lgbm_params = json.load(f)\n",
        "    print(\"✅ Лучшие параметры успешно загружены:\")\n",
        "    print(best_lgbm_params)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Ошибка загрузки файла с параметрами: {e}\")\n",
        "\n",
        "# --- 2. Перезапуск ансамбля с новыми параметрами ---\n",
        "if 'df_processed_advanced' in locals() and 'best_lgbm_params' in locals():\n",
        "    print(\"\\n--- Перезапускаю обучение ансамбля с ОПТИМИЗИРОВАННЫМИ параметрами ---\")\n",
        "\n",
        "    # Подготовка данных\n",
        "    y = df_processed_advanced['resolution']\n",
        "    X_tab = df_processed_advanced.drop(columns=['resolution', 'id', 'description', 'name_rus', 'description_cleaned', 'name_rus_cleaned', 'text_features'])\n",
        "    X_text = df_processed_advanced['text_features']\n",
        "    categorical_features = ['brand_name', 'CommercialTypeName4']\n",
        "    for col in categorical_features:\n",
        "        X_tab[col] = X_tab[col].astype('category')\n",
        "\n",
        "    N_SPLITS = 5\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "    oof_lgbm = np.zeros(len(X_tab))\n",
        "    oof_text = np.zeros(len(X_tab))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_tab, y)):\n",
        "        print(f\"--- Фолд {fold+1}/{N_SPLITS} ---\")\n",
        "        X_train_tab, y_train = X_tab.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_val_tab, y_val = X_tab.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Обучение ОПТИМИЗИРОВАННОГО LightGBM\n",
        "        final_params = best_lgbm_params.copy()\n",
        "        neg_count, pos_count = y_train.value_counts()\n",
        "        final_params.update({\n",
        "            'objective': 'binary', 'metric': 'f1', 'verbosity': -1,\n",
        "            'random_state': 42, 'n_estimators': 2000,\n",
        "            'scale_pos_weight': neg_count / pos_count\n",
        "        })\n",
        "        lgbm_model = LGBMClassifier(**final_params)\n",
        "        lgbm_model.fit(X_train_tab, y_train, eval_set=[(X_val_tab, y_val)], eval_metric='f1', callbacks=[]) # early_stopping(100)\n",
        "        oof_lgbm[val_idx] = lgbm_model.predict_proba(X_val_tab)[:, 1]\n",
        "\n",
        "        # Обучение текстовой модели\n",
        "        X_train_text, X_val_text = X_text.iloc[train_idx], X_text.iloc[val_idx]\n",
        "\n",
        "        # --- ИСПРАВЛЕНИЕ: Определяем пайплайн ВНУТРИ цикла ---\n",
        "        text_pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=3, max_df=0.9)),\n",
        "            ('logreg', LogisticRegression(C=5, class_weight='balanced', random_state=42, solver='liblinear'))\n",
        "        ])\n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        text_pipeline.fit(X_train_text, y_train)\n",
        "        oof_text[val_idx] = text_pipeline.predict_proba(X_val_text)[:, 1]\n",
        "\n",
        "    # --- 3. Поиск лучшего веса и порога ---\n",
        "    print(\"\\nПодбираю лучший вес и порог для ФИНАЛЬНОГО ансамбля...\")\n",
        "    best_f1, best_weight, best_threshold = 0, 0, 0\n",
        "    for weight in np.arange(0.6, 1.01, 0.05):\n",
        "        blended_oof = weight * oof_lgbm + (1 - weight) * oof_text\n",
        "        for threshold in np.arange(0.1, 0.91, 0.05):\n",
        "            preds = (blended_oof > threshold).astype(int)\n",
        "            f1 = f1_score(y, preds)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_weight, best_threshold = f1, weight, threshold\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Подбор для ОПТИМИЗИРОВАННОЙ модели завершен!\")\n",
        "    print(f\"Наш прошлый лучший результат: 0.8328\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"НОВЫЙ лучший F1-score ансамбля: {best_f1:.4f}\")\n",
        "    print(f\"Лучший вес для LGBM: {best_weight:.2f}\")\n",
        "    print(f\"Лучший порог: {best_threshold:.2f}\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"❌ Необходимые данные (df_processed_advanced) не найдены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkXLleqGJWA_",
        "outputId": "cb170f34-1857-4cac-db30-bbf893f4b772"
      },
      "id": "DkXLleqGJWA_",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Загружаю лучшие параметры, найденные Optuna ---\n",
            "Mounted at /content/drive\n",
            "✅ Лучшие параметры успешно загружены:\n",
            "{'learning_rate': 0.05905048835314297, 'num_leaves': 263, 'max_depth': 12, 'min_child_samples': 7, 'subsample': 0.8288864010163431, 'colsample_bytree': 0.6048620922810927, 'reg_alpha': 0.156919415649457, 'reg_lambda': 0.8711271060286511}\n",
            "\n",
            "--- Перезапускаю обучение ансамбля с ОПТИМИЗИРОВАННЫМИ параметрами ---\n",
            "--- Фолд 1/5 ---\n",
            "--- Фолд 2/5 ---\n",
            "--- Фолд 3/5 ---\n",
            "--- Фолд 4/5 ---\n",
            "--- Фолд 5/5 ---\n",
            "\n",
            "Подбираю лучший вес и порог для ФИНАЛЬНОГО ансамбля...\n",
            "\n",
            "==================================================\n",
            "Подбор для ОПТИМИЗИРОВАННОЙ модели завершен!\n",
            "Наш прошлый лучший результат: 0.8328\n",
            "--------------------\n",
            "НОВЫЙ лучший F1-score ансамбля: 0.8477\n",
            "Лучший вес для LGBM: 0.60\n",
            "Лучший порог: 0.55\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6871b8f7",
      "metadata": {
        "id": "6871b8f7"
      },
      "source": [
        "## Обучение и формирование submission.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be3dbf8",
      "metadata": {
        "id": "4be3dbf8"
      },
      "source": [
        "### --- 1. Загрузка и полная обработка данных ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff14d59",
      "metadata": {
        "id": "9ff14d59"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "print(\"--- Создаю финальный submission-файл с лучшей моделью ---\")\n",
        "print(\"1. Загружаю и обрабатываю данные...\")\n",
        "\n",
        "\n",
        "df_train = pd.read_csv('data/raw/ml_ozon_сounterfeit_train.csv')\n",
        "df_test = pd.read_csv('data/raw/ml_ozon_сounterfeit_test.csv')\n",
        "\n",
        "test_ids = df_test['id']\n",
        "\n",
        "train_processed = preprocess_data(df_train)\n",
        "train_final = create_advanced_features(train_processed)\n",
        "\n",
        "test_processed = preprocess_data(df_test)\n",
        "test_final = create_advanced_features(test_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e7c83e",
      "metadata": {
        "id": "e2e7c83e"
      },
      "source": [
        "### --- 2. Обучение финальных моделей на ВСЕХ данных ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973b5873",
      "metadata": {
        "id": "973b5873"
      },
      "outputs": [],
      "source": [
        "required_vars = ['train_final', 'test_final']\n",
        "error = [var for var in required_vars if var not in locals()]\n",
        "\n",
        "if not error:\n",
        "    print(\"2. Обучаю финальные модели...\")\n",
        "\n",
        "    # LightGBM\n",
        "    train_final['text_features'] = train_final['name_rus_cleaned'] + ' ' + train_final['description_cleaned']\n",
        "    features_to_drop = ['resolution', 'id', 'description', 'name_rus', 'description_cleaned', 'name_rus_cleaned', 'text_features']\n",
        "    X_train_tab = train_final.drop(columns=features_to_drop)\n",
        "    y_train = train_final['resolution']\n",
        "\n",
        "    test_final['text_features'] = test_final['name_rus_cleaned'] + ' ' + test_final['description_cleaned']\n",
        "    X_test_tab = test_final.drop(columns=[col for col in features_to_drop if col in test_final.columns])\n",
        "\n",
        "    X_train_tab, X_test_tab = X_train_tab.align(X_test_tab, join='left', axis=1, fill_value=0)\n",
        "    categorical_features = ['brand_name', 'CommercialTypeName4']\n",
        "    for col in categorical_features:\n",
        "        X_train_tab[col] = X_train_tab[col].astype('category')\n",
        "        X_test_tab[col] = X_test_tab[col].astype('category')\n",
        "\n",
        "    neg_count, pos_count = y_train.value_counts()\n",
        "    lgbm_model = LGBMClassifier(random_state=42, scale_pos_weight=neg_count / pos_count, n_estimators=1000, learning_rate=0.05, num_leaves=31)\n",
        "    lgbm_model.fit(X_train_tab, y_train, categorical_feature=categorical_features)\n",
        "    print(\"   ...LGBM обучен.\")\n",
        "\n",
        "    # Текстовая модель\n",
        "    X_train_text = train_final['text_features']\n",
        "    X_test_text = test_final['text_features']\n",
        "\n",
        "    text_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=3, max_df=0.9)),\n",
        "        ('logreg', LogisticRegression(C=5, class_weight='balanced', random_state=42, solver='liblinear'))\n",
        "    ])\n",
        "    text_pipeline.fit(X_train_text, y_train)\n",
        "    print(\"   ...Текстовая модель обучена.\")\n",
        "else:\n",
        "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befd1465",
      "metadata": {
        "id": "befd1465"
      },
      "source": [
        "### --- 3. Предсказание и блендинг ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d514f255",
      "metadata": {
        "id": "d514f255"
      },
      "outputs": [],
      "source": [
        "required_vars = ['lgbm_model', 'text_pipeline']\n",
        "error = [var for var in required_vars if var not in locals()]\n",
        "\n",
        "if not error:\n",
        "    print(\"3. Делаю предсказания и смешиваю их...\")\n",
        "    lgbm_probs = lgbm_model.predict_proba(X_test_tab)[:, 1]\n",
        "    text_probs = text_pipeline.predict_proba(X_test_text)[:, 1]\n",
        "\n",
        "    LGBM_WEIGHT = 0.70\n",
        "    TEXT_WEIGHT = 1 - LGBM_WEIGHT\n",
        "    blended_probs = LGBM_WEIGHT * lgbm_probs + TEXT_WEIGHT * text_probs\n",
        "else:\n",
        "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5820cd",
      "metadata": {
        "id": "7a5820cd"
      },
      "source": [
        "### --- 4. Применение порога и сохранение ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e697fa",
      "metadata": {
        "id": "96e697fa"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "required_vars = ['blended_probs']\n",
        "error = [var for var in required_vars if var not in locals()]\n",
        "\n",
        "if not error:\n",
        "    print(\"4. Применяю порог и сохраняю результат...\")\n",
        "    BEST_THRESHOLD = 0.70\n",
        "    predictions = (blended_probs > BEST_THRESHOLD).astype(int)\n",
        "\n",
        "    submission = pd.DataFrame({'id': test_ids, 'prediction': predictions})\n",
        "\n",
        "    # Создаём имя файла с меткой времени\n",
        "    timestamp = datetime.now().strftime(\"%m.%d_%H:%M\")\n",
        "    filename = f\"submission_{timestamp}.csv\"\n",
        "    # Сохраняем\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(\"✅ Финальный сабмит 'submission.csv' готов к отправке!\")\n",
        "else:\n",
        "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f0644406",
        "3dfa956d",
        "9c102d3f",
        "6871b8f7"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
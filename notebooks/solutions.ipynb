{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0644406",
   "metadata": {},
   "source": [
    "# Настраевае среду colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03733eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725bba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%git clone https://github.com/meineac/counterfeit-ozon-ml.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd counterfeit-ozon-ml\\notebooks\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f3bed",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5badf6c9",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7fb084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Тренировочный файл успешно загружен.\n",
      "\n",
      "--- 1. Размерность данных ---\n",
      "Форма датасета: (197198, 45)\n",
      "\n",
      "--- 2. Общая информация и типы данных ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 197198 entries, 0 to 197197\n",
      "Data columns (total 45 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   id                            197198 non-null  int64  \n",
      " 1   resolution                    197198 non-null  int64  \n",
      " 2   brand_name                    116667 non-null  object \n",
      " 3   description                   171138 non-null  object \n",
      " 4   name_rus                      197198 non-null  object \n",
      " 5   CommercialTypeName4           197198 non-null  object \n",
      " 6   rating_1_count                47193 non-null   float64\n",
      " 7   rating_2_count                47193 non-null   float64\n",
      " 8   rating_3_count                47193 non-null   float64\n",
      " 9   rating_4_count                47193 non-null   float64\n",
      " 10  rating_5_count                47193 non-null   float64\n",
      " 11  comments_published_count      47193 non-null   float64\n",
      " 12  photos_published_count        47193 non-null   float64\n",
      " 13  videos_published_count        47193 non-null   float64\n",
      " 14  PriceDiscounted               197198 non-null  float64\n",
      " 15  item_time_alive               197198 non-null  int64  \n",
      " 16  item_count_fake_returns7      197198 non-null  int64  \n",
      " 17  item_count_fake_returns30     197198 non-null  int64  \n",
      " 18  item_count_fake_returns90     197198 non-null  int64  \n",
      " 19  item_count_sales7             197198 non-null  int64  \n",
      " 20  item_count_sales30            197198 non-null  int64  \n",
      " 21  item_count_sales90            197198 non-null  int64  \n",
      " 22  item_count_returns7           197198 non-null  int64  \n",
      " 23  item_count_returns30          197198 non-null  int64  \n",
      " 24  item_count_returns90          197198 non-null  int64  \n",
      " 25  GmvTotal7                     187007 non-null  float64\n",
      " 26  GmvTotal30                    189268 non-null  float64\n",
      " 27  GmvTotal90                    189791 non-null  float64\n",
      " 28  ExemplarAcceptedCountTotal7   187007 non-null  float64\n",
      " 29  ExemplarAcceptedCountTotal30  189268 non-null  float64\n",
      " 30  ExemplarAcceptedCountTotal90  189791 non-null  float64\n",
      " 31  OrderAcceptedCountTotal7      186797 non-null  float64\n",
      " 32  OrderAcceptedCountTotal30     189038 non-null  float64\n",
      " 33  OrderAcceptedCountTotal90     189681 non-null  float64\n",
      " 34  ExemplarReturnedCountTotal7   187007 non-null  float64\n",
      " 35  ExemplarReturnedCountTotal30  189268 non-null  float64\n",
      " 36  ExemplarReturnedCountTotal90  189791 non-null  float64\n",
      " 37  ExemplarReturnedValueTotal7   187007 non-null  float64\n",
      " 38  ExemplarReturnedValueTotal30  189268 non-null  float64\n",
      " 39  ExemplarReturnedValueTotal90  189791 non-null  float64\n",
      " 40  ItemVarietyCount              196201 non-null  float64\n",
      " 41  ItemAvailableCount            196201 non-null  float64\n",
      " 42  seller_time_alive             197198 non-null  float64\n",
      " 43  ItemID                        197198 non-null  int64  \n",
      " 44  SellerID                      197198 non-null  int64  \n",
      "dtypes: float64(27), int64(14), object(4)\n",
      "memory usage: 67.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_folder = '../data/raw/'\n",
    "file_name = 'ml_ozon_counterfeit_train.csv'\n",
    "file_path = file_folder + file_name \n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(file_path)\n",
    "    print(\"✅ Тренировочный файл успешно загружен.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Ошибка: Файл не найден по пути '{file_path}'. Проверьте путь.\")\n",
    "\n",
    "if 'df_train' in locals():\n",
    "    print(\"\\n--- 1. Размерность данных ---\")\n",
    "    print(f\"Форма датасета: {df_train.shape}\")\n",
    "    \n",
    "    print(\"\\n--- 2. Общая информация и типы данных ---\")\n",
    "    # .info() показывает типы данных и количество НЕпустых значений\n",
    "    df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa956d",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2005d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начинаю Шаг 3: Предобработка и инжиниринг признаков ---\n",
      "1. Заполняю пропуски...\n",
      "   ...пропуски заполнены.\n",
      "2. Очищаю текстовые поля от HTML-тегов...\n",
      "   ...текст очищен.\n",
      "3. Создаю новые признаки...\n",
      "   ...новые признаки созданы.\n",
      "\n",
      "--- Проверка результата ---\n",
      "Оставшиеся пропуски:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "✅ Шаг 3 успешно завершен!\n"
     ]
    }
   ],
   "source": [
    "import re # Импортируем модуль для работы с регулярными выражениями\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"--- Начинаю Шаг 3: Предобработка и инжиниринг признаков ---\")\n",
    "    \n",
    "    # Создаем копию, чтобы не изменять оригинальный датафрейм\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- 1. Обработка пропущенных значений ---\n",
    "    print(\"1. Заполняю пропуски...\")\n",
    "    \n",
    "    # Блок с рейтингами и отзывами (NaN -> 0)\n",
    "    rating_cols = [col for col in df_processed.columns if 'rating' in col or 'count' in col]\n",
    "    # Уточняем список, чтобы не затронуть лишние столбцы\n",
    "    cols_to_fill_zero = [\n",
    "        'rating_1_count', 'rating_2_count', 'rating_3_count', 'rating_4_count', 'rating_5_count',\n",
    "        'comments_published_count', 'photos_published_count', 'videos_published_count',\n",
    "        'GmvTotal7', 'GmvTotal30', 'GmvTotal90', 'ExemplarAcceptedCountTotal7',\n",
    "        'ExemplarAcceptedCountTotal30', 'ExemplarAcceptedCountTotal90', 'OrderAcceptedCountTotal7',\n",
    "        'OrderAcceptedCountTotal30', 'OrderAcceptedCountTotal90', 'ExemplarReturnedCountTotal7',\n",
    "        'ExemplarReturnedCountTotal30', 'ExemplarReturnedCountTotal90', 'ExemplarReturnedValueTotal7',\n",
    "        'ExemplarReturnedValueTotal30', 'ExemplarReturnedValueTotal90', 'ItemVarietyCount', 'ItemAvailableCount'\n",
    "    ]\n",
    "    for col in cols_to_fill_zero:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = df_processed[col].fillna(0)\n",
    "    \n",
    "    # Категориальные признаки\n",
    "    df_processed['brand_name'] = df_processed['brand_name'].fillna('_UNKNOWN_')\n",
    "    df_processed['description'] = df_processed['description'].fillna('no_description')\n",
    "    \n",
    "    print(\"   ...пропуски заполнены.\")\n",
    "\n",
    "    # --- 2. Очистка текста ---\n",
    "    print(\"2. Очищаю текстовые поля от HTML-тегов...\")\n",
    "    def clean_html(text):\n",
    "        if isinstance(text, str):\n",
    "            # Удаляем HTML-теги\n",
    "            clean_text = re.sub(r'<.*?>', ' ', text)\n",
    "            # Удаляем переносы строк и лишние пробелы\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "            return clean_text\n",
    "        return text\n",
    "\n",
    "    df_processed['description_cleaned'] = df_processed['description'].apply(clean_html)\n",
    "    df_processed['name_rus_cleaned'] = df_processed['name_rus'].apply(clean_html)\n",
    "    print(\"   ...текст очищен.\")\n",
    "\n",
    "    # --- 3. Feature Engineering ---\n",
    "    print(\"3. Создаю новые признаки...\")\n",
    "    \n",
    "    # Признаки на основе длины текста\n",
    "    df_processed['description_len'] = df_processed['description_cleaned'].str.len()\n",
    "    df_processed['name_rus_len'] = df_processed['name_rus_cleaned'].str.len()\n",
    "    \n",
    "    # Признаки-отношения (добавляем epsilon для избежания деления на ноль)\n",
    "    epsilon = 1e-6\n",
    "    df_processed['return_to_sales_ratio_90'] = df_processed['item_count_returns90'] / (df_processed['item_count_sales90'] + epsilon)\n",
    "    df_processed['fake_return_ratio_90'] = df_processed['item_count_fake_returns90'] / (df_processed['item_count_returns90'] + epsilon)\n",
    "    \n",
    "    # Признак \"есть ли у товара отзывы\"\n",
    "    df_processed['has_reviews'] = (df_processed['rating_1_count'] + df_processed['rating_5_count'] > 0).astype(int)\n",
    "    print(\"   ...новые признаки созданы.\")\n",
    "    \n",
    "    print(\"\\n--- Проверка результата ---\")\n",
    "    # Проверяем, что пропусков не осталось (кроме тех, где изначально не было)\n",
    "    remaining_na = df_processed.isnull().sum()\n",
    "    print(\"Оставшиеся пропуски:\")\n",
    "    print(remaining_na[remaining_na > 0])\n",
    "    print(\"\\n✅ Шаг 3 успешно завершен!\")\n",
    "    return df_processed\n",
    "\n",
    "if 'df_train' in locals():\n",
    "    df_processed = preprocess_data(df_train)\n",
    "else:\n",
    "    print(\"❌ Переменная 'df_train' не найдена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e90309",
   "metadata": {},
   "source": [
    "### Сохраняем обработанные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7deee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Обработанные данные успешно сохранены в файл:\n",
      "   -> ../data/processed/df_processed.feather\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'df_processed' in locals():\n",
    "    # --- ШАГ 1: Определяем путь для сохранения ---\n",
    "    # Создаем папку 'data/processed/', если она еще не существует\n",
    "    processed_data_path = '../data/processed/'\n",
    "    os.makedirs(processed_data_path, exist_ok=True)\n",
    "    \n",
    "    # Имя файла\n",
    "    output_file_path = os.path.join(processed_data_path, 'df_processed.feather')\n",
    "\n",
    "    # --- ШАГ 2: Сохраняем DataFrame ---\n",
    "    try:\n",
    "        # Feather требует сбросить индекс, если он не является стандартным RangeIndex\n",
    "        df_processed.reset_index(drop=True).to_feather(output_file_path)\n",
    "        print(f\"✅ Обработанные данные успешно сохранены в файл:\")\n",
    "        print(f\"   -> {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Произошла ошибка при сохранении файла: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Переменная 'df_processed' не найдена. Пожалуйста, выполните предыдущий шаг.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c102d3f",
   "metadata": {},
   "source": [
    "## Расшириные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9db410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"   ...создаю продвинутые признаки...\")\n",
    "    \n",
    "    # 1. Признак на основе расстояния Левенштейна\n",
    "    # Сравниваем бренд из метаданных с текстом в названии товара\n",
    "    # fuzz.partial_ratio находит лучшее совпадение подстроки\n",
    "    df['brand_name_match_score'] = df.apply(\n",
    "        lambda row: fuzz.partial_ratio(str(row['brand_name']).lower(), str(row['name_rus_cleaned']).lower()),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 2. Признаки на основе ключевых слов\n",
    "    suspicious_words = ['копия', 'реплика', 'аналог', 'imitate', 'replica', 'copy']\n",
    "    for word in suspicious_words:\n",
    "        df[f'has_word_{word}'] = df['description_cleaned'].str.contains(word, case=False).astype(int)\n",
    "\n",
    "    # 3. Статистические признаки текста\n",
    "    df['desc_uppercase_ratio'] = df['description_cleaned'].str.count(r'[A-ZА-Я]') / (df['description_len'] + 1e-6)\n",
    "    df['desc_digit_ratio'] = df['description_cleaned'].str.count(r'[0-9]') / (df['description_len'] + 1e-6)\n",
    "    \n",
    "    print(\"   ...продвинутые признаки созданы.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b390f",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129576f7",
   "metadata": {},
   "source": [
    "### # --- 1. Подготовка данных ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...создаю продвинутые признаки...\n",
      "   ...продвинутые признаки созданы.\n",
      "\n",
      "--- Перезапускаю обучение ансамбля с новыми признаками ---\n",
      "\n",
      "Получаю OOF-предсказания на 5 фолдах (LGBM будет использовать новые фичи)...\n",
      "--- Фолд 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\counterfeit-ozon-ml\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:131: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\counterfeit-ozon-ml\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "  File \"d:\\counterfeit-ozon-ml\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\matve\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\matve\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\matve\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 10442, number of negative: 147316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10966\n",
      "[LightGBM] [Info] Number of data points in the train set: 157758, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066190 -> initscore=-2.646744\n",
      "[LightGBM] [Info] Start training from score -2.646744\n"
     ]
    }
   ],
   "source": [
    "# --- Применяем новую функцию к нашим данным ---\n",
    "if 'df_processed' in locals():\n",
    "    df_processed_advanced = create_advanced_features(df_processed)\n",
    "    \n",
    "    print(\"\\n--- Перезапускаю обучение ансамбля с новыми признаками ---\")\n",
    "\n",
    "    df_processed_advanced['text_features'] = df_processed_advanced['name_rus_cleaned'] + ' ' + df_processed_advanced['description_cleaned']\n",
    "    y = df_processed_advanced['resolution']\n",
    "    \n",
    "    features_to_drop = ['resolution', 'id', 'description', 'name_rus', 'description_cleaned', 'name_rus_cleaned', 'text_features']\n",
    "    X_tab = df_processed_advanced.drop(columns=features_to_drop)\n",
    "    categorical_features = ['brand_name', 'CommercialTypeName4']\n",
    "    for col in categorical_features:\n",
    "        X_tab[col] = X_tab[col].astype('category')\n",
    "        \n",
    "    X_text = df_processed_advanced['text_features']\n",
    "else:\n",
    "    print(\"❌ Переменная 'df_processed' не найдена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c75da",
   "metadata": {},
   "source": [
    "### --- 2. Кросс-валидация ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_vars = ['y', 'X_tab', 'X_text', 'categorical_features']\n",
    "error = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if not error:\n",
    "    N_SPLITS = 5\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_lgbm = np.zeros(len(X_tab))\n",
    "    oof_text = np.zeros(len(X_tab))\n",
    "\n",
    "    print(f\"\\nПолучаю OOF-предсказания на {N_SPLITS} фолдах (LGBM будет использовать новые фичи)...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_tab, y)):\n",
    "        print(f\"--- Фолд {fold+1}/{N_SPLITS} ---\")\n",
    "        \n",
    "        # Обучение LightGBM\n",
    "        X_train_tab, y_train = X_tab.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val_tab, y_val = X_tab.iloc[val_idx], y.iloc[val_idx]\n",
    "        neg_count, pos_count = y_train.value_counts()\n",
    "        lgbm_model = LGBMClassifier(random_state=42, scale_pos_weight=neg_count / pos_count, n_estimators=1000, learning_rate=0.05, num_leaves=31)\n",
    "        lgbm_model.fit(X_train_tab, y_train, eval_set=[(X_val_tab, y_val)], eval_metric='f1', callbacks=[], categorical_feature=categorical_features)\n",
    "        oof_lgbm[val_idx] = lgbm_model.predict_proba(X_val_tab)[:, 1]\n",
    "\n",
    "        # Обучение Text Pipeline\n",
    "        X_train_text, X_val_text = X_text.iloc[train_idx], X_text.iloc[val_idx]\n",
    "        text_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=3, max_df=0.9)),\n",
    "            ('logreg', LogisticRegression(C=5, class_weight='balanced', random_state=42, solver='liblinear'))\n",
    "        ])\n",
    "        text_pipeline.fit(X_train_text, y_train)\n",
    "        oof_text[val_idx] = text_pipeline.predict_proba(X_val_text)[:, 1]\n",
    "else:\n",
    "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397e8e6",
   "metadata": {},
   "source": [
    "### --- 3. Поиск лучшего веса и порога ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1e0e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3504710579.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif :\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "required_vars = ['oof_lgbm', 'oof_text']\n",
    "error = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if not error:\n",
    "    print(\"\\nПодбираю лучший вес и порог для НОВОГО ансамбля...\")\n",
    "    best_f1, best_weight, best_threshold = 0, 0, 0\n",
    "    for weight in np.arange(0.6, 1.01, 0.05):\n",
    "        blended_oof = weight * oof_lgbm + (1 - weight) * oof_text\n",
    "        for threshold in np.arange(0.1, 0.91, 0.05):\n",
    "            preds = (blended_oof > threshold).astype(int)\n",
    "            f1 = f1_score(y, preds)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_weight, best_threshold = f1, weight, threshold\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Подбор для УЛУЧШЕННОЙ модели завершен!\")\n",
    "    print(f\"F1-score УЛУЧШЕННОЙ табличной модели (LGBM): {f1_score(y, (oof_lgbm > 0.5).astype(int)):.4f}\")\n",
    "    print(f\"F1-score текстовой модели (LogReg): {f1_score(y, (oof_text > 0.5).astype(int)):.4f}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Лучший F1-score НОВОГО ансамбля: {best_f1:.4f}\")\n",
    "    print(f\"Лучший вес для LGBM: {best_weight:.2f}\")\n",
    "    print(f\"Лучший порог: {best_threshold:.2f}\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871b8f7",
   "metadata": {},
   "source": [
    "### Обучение и формирование submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3dbf8",
   "metadata": {},
   "source": [
    "#### --- 1. Загрузка и полная обработка данных ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff14d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Создаю финальный submission-файл с лучшей моделью ---\")\n",
    "\n",
    "\n",
    "print(\"1. Загружаю и обрабатываю данные...\")\n",
    "df_train = pd.read_csv('ml_ozon_сounterfeit_train.csv')\n",
    "df_test = pd.read_csv('ml_ozon_сounterfeit_test.csv')\n",
    "test_ids = df_test['id']\n",
    "\n",
    "train_processed = preprocess_data(df_train)\n",
    "train_final = create_advanced_features(train_processed)\n",
    "\n",
    "test_processed = preprocess_data(df_test)\n",
    "test_final = create_advanced_features(test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7c83e",
   "metadata": {},
   "source": [
    "#### --- 2. Обучение финальных моделей на ВСЕХ данных ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_vars = ['train_final', 'test_final']\n",
    "error = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if not error:\n",
    "    print(\"2. Обучаю финальные модели...\")\n",
    "\n",
    "    # LightGBM\n",
    "    train_final['text_features'] = train_final['name_rus_cleaned'] + ' ' + train_final['description_cleaned']\n",
    "    features_to_drop = ['resolution', 'id', 'description', 'name_rus', 'description_cleaned', 'name_rus_cleaned', 'text_features']\n",
    "    X_train_tab = train_final.drop(columns=features_to_drop)\n",
    "    y_train = train_final['resolution']\n",
    "\n",
    "    test_final['text_features'] = test_final['name_rus_cleaned'] + ' ' + test_final['description_cleaned']\n",
    "    X_test_tab = test_final.drop(columns=[col for col in features_to_drop if col in test_final.columns])\n",
    "\n",
    "    X_train_tab, X_test_tab = X_train_tab.align(X_test_tab, join='left', axis=1, fill_value=0)\n",
    "    categorical_features = ['brand_name', 'CommercialTypeName4']\n",
    "    for col in categorical_features:\n",
    "        X_train_tab[col] = X_train_tab[col].astype('category')\n",
    "        X_test_tab[col] = X_test_tab[col].astype('category')\n",
    "\n",
    "    neg_count, pos_count = y_train.value_counts()\n",
    "    lgbm_model = LGBMClassifier(random_state=42, scale_pos_weight=neg_count / pos_count, n_estimators=1000, learning_rate=0.05, num_leaves=31)\n",
    "    lgbm_model.fit(X_train_tab, y_train, categorical_feature=categorical_features)\n",
    "    print(\"   ...LGBM обучен.\")\n",
    "\n",
    "    # Текстовая модель\n",
    "    X_train_text = train_final['text_features']\n",
    "    X_test_text = test_final['text_features']\n",
    "\n",
    "    text_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=3, max_df=0.9)),\n",
    "        ('logreg', LogisticRegression(C=5, class_weight='balanced', random_state=42, solver='liblinear'))\n",
    "    ])\n",
    "    text_pipeline.fit(X_train_text, y_train)\n",
    "    print(\"   ...Текстовая модель обучена.\")\n",
    "else:\n",
    "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd1465",
   "metadata": {},
   "source": [
    "#### --- 3. Предсказание и блендинг ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_vars = ['lgbm_model', 'text_pipeline']\n",
    "error = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if not error:\n",
    "    print(\"3. Делаю предсказания и смешиваю их...\")\n",
    "    lgbm_probs = lgbm_model.predict_proba(X_test_tab)[:, 1]\n",
    "    text_probs = text_pipeline.predict_proba(X_test_text)[:, 1]\n",
    "\n",
    "    LGBM_WEIGHT = 0.70\n",
    "    TEXT_WEIGHT = 1 - LGBM_WEIGHT\n",
    "    blended_probs = LGBM_WEIGHT * lgbm_probs + TEXT_WEIGHT * text_probs\n",
    "else:\n",
    "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5820cd",
   "metadata": {},
   "source": [
    "#### --- 4. Применение порога и сохранение ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e697fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Применяю порог и сохраняю результат...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'blended_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4. Применяю порог и сохраняю результат...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m BEST_THRESHOLD = \u001b[32m0.70\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m predictions = (\u001b[43mblended_probs\u001b[49m > BEST_THRESHOLD).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      5\u001b[39m submission = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: test_ids, \u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m: predictions})\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Создаём имя файла с меткой времени\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'blended_probs' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "required_vars = ['blended_probs']\n",
    "error = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if not error:\n",
    "    print(\"4. Применяю порог и сохраняю результат...\")\n",
    "    BEST_THRESHOLD = 0.70\n",
    "    predictions = (blended_probs > BEST_THRESHOLD).astype(int)\n",
    "\n",
    "    submission = pd.DataFrame({'id': test_ids, 'prediction': predictions})\n",
    "\n",
    "    # Создаём имя файла с меткой времени\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    filename = f\"submission_{timestamp}.csv\"\n",
    "    # Сохраняем\n",
    "    submission.to_csv(filename, index=False)\n",
    "\n",
    "    print(\"✅ Финальный сабмит 'submission.csv' готов к отправке!\")\n",
    "else:\n",
    "    print(f\"❌ Не найдены переменные: {', '.join(error)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
